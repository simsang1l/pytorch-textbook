{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageTransform():\n",
    "    def __init__(self, resize, mean, std):\n",
    "        self.data_transform= {\n",
    "            'train': transforms.Compose([\n",
    "                transforms.RandomResizedCrop(resize, scale=(0.5, 1.0)),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean, std)\n",
    "            ]),\n",
    "            'val': transforms.Compose([\n",
    "                transforms.Resize(256),\n",
    "                transforms.CenterCrop(resize),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean, std)\n",
    "            ])\n",
    "        }\n",
    "        \n",
    "    def __call__(self, img, phase):\n",
    "        return self.data_transform[phase](img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400 92 10\n"
     ]
    }
   ],
   "source": [
    "cat_directory = r\"./data/dogs-vs-cats/Cat\"\n",
    "dog_directory = r\"./data/dogs-vs-cats/Dog\"\n",
    "\n",
    "cat_images_filepaths = sorted([os.path.join(cat_directory, f) for f in os.listdir(cat_directory)])\n",
    "dog_images_filepaths = sorted([os.path.join(dog_directory, f) for f in os.listdir(dog_directory)])\n",
    "images_filepaths = [*cat_images_filepaths, *dog_images_filepaths]\n",
    "correct_images_filepaths = [i for i in images_filepaths if cv2.imread(i) is not None]\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(correct_images_filepaths)\n",
    "train_images_filepaths = correct_images_filepaths[:400]\n",
    "val_images_filepaths = correct_images_filepaths[400:-10]\n",
    "test_images_filepaths = correct_images_filepaths[-10:]\n",
    "print(len(train_images_filepaths), len(val_images_filepaths), len(test_images_filepaths))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image_grid(images_filepaths, predicted_labels=(), cols = 5):\n",
    "    rows = len(images_filepaths) // cols\n",
    "    figure, ax = plt.subplots(nrows = rows, ncols=cols, figsize=(12, 6))\n",
    "    for i, image_filepath in enumerate(images_filepaths):\n",
    "        print('path;;', image_filepath)\n",
    "        image = cv2.imread(image_filepath)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        true_label = os.path.normpath(image_filepath).split(os.sep)[-2]\n",
    "        predicted_label = predicted_labels[i] if predicted_labels else true_label\n",
    "        color = \"green\" if true_label == predicted_label else \"red\"\n",
    "        ax.ravel()[i].set_title(image)\n",
    "        ax.ravel()[i].set_title(predicted_label, color = color)\n",
    "        ax.ravel()[i].set_axis_off()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path;; ./data/dogs-vs-cats/Cat\\cat.145.jpg\n",
      "path;; ./data/dogs-vs-cats/Dog\\dog.211.jpg\n",
      "path;; ./data/dogs-vs-cats/Cat\\cat.162.jpg\n",
      "path;; ./data/dogs-vs-cats/Cat\\cat.200.jpg\n",
      "path;; ./data/dogs-vs-cats/Cat\\cat.210.jpg\n",
      "path;; ./data/dogs-vs-cats/Cat\\cat.224.jpg\n",
      "path;; ./data/dogs-vs-cats/Dog\\dog.213.jpg\n",
      "path;; ./data/dogs-vs-cats/Cat\\cat.109.jpg\n",
      "path;; ./data/dogs-vs-cats/Cat\\cat.15.jpg\n",
      "path;; ./data/dogs-vs-cats/Dog\\dog.167.jpg\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_image_grid(test_images_filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DogvsCatDataset(Dataset):\n",
    "    def __init__(self, file_list, transform=None, phase='train'):\n",
    "        self.file_list = file_list\n",
    "        self.transform = transform\n",
    "        self.phase = phase\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.file_list[idx]\n",
    "        img = Image.open(img_path)\n",
    "        img_transformed = self.transform(img, self.phase)\n",
    "        \n",
    "        label = img_path.split('/')[-1].split('.')[0]\n",
    "        if label == 'dog':\n",
    "            label = 1\n",
    "        elif label == 'cat':\n",
    "            label = 0\n",
    "      \n",
    "        return img_transformed, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 224\n",
    "mean = (0.485, 0.456, 0.406)\n",
    "std = (0.229, 0.224, 0.225)\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 224, 224])\n",
      "Cat\\cat\n"
     ]
    }
   ],
   "source": [
    "train_dataset = DogvsCatDataset(train_images_filepaths, transform = ImageTransform(size, mean, std), phase = 'train')\n",
    "val_dataset = DogvsCatDataset(val_images_filepaths, transform = ImageTransform(size, mean, std), phase = 'val')\n",
    "\n",
    "index = 0\n",
    "print(train_dataset.__getitem__(index)[0].size())\n",
    "print(train_dataset.__getitem__(index)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 224, 224])\n",
      "('Dog\\\\dog', 'Cat\\\\cat', 'Cat\\\\cat', 'Dog\\\\dog', 'Cat\\\\cat', 'Cat\\\\cat', 'Cat\\\\cat', 'Dog\\\\dog', 'Cat\\\\cat', 'Dog\\\\dog', 'Dog\\\\dog', 'Dog\\\\dog', 'Cat\\\\cat', 'Dog\\\\dog', 'Dog\\\\dog', 'Dog\\\\dog', 'Dog\\\\dog', 'Dog\\\\dog', 'Cat\\\\cat', 'Cat\\\\cat', 'Dog\\\\dog', 'Dog\\\\dog', 'Cat\\\\cat', 'Dog\\\\dog', 'Dog\\\\dog', 'Cat\\\\cat', 'Dog\\\\dog', 'Cat\\\\cat', 'Cat\\\\cat', 'Cat\\\\cat', 'Dog\\\\dog', 'Dog\\\\dog')\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size = batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size = batch_size, shuffle=False)\n",
    "dataloader_dict = {\"train\": train_dataloader, \"val\": val_dataloader}\n",
    "\n",
    "batch_iterator = iter(train_dataloader)\n",
    "inputs, label = next(batch_iterator)\n",
    "print(inputs.size())\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.cnn1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=5, stride=1, padding = 0)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.cnn2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride = 1, padding = 0)\n",
    "        \n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(32*53*53, 512)\n",
    "        self.relu5 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(512, 2)\n",
    "        self.output = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.cnn1(x)\n",
    "        out = self.relu1(out)\n",
    "        out = self.maxpool1(out)\n",
    "        out = self.cnn2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.maxpool2(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        \n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.output(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet(\n",
      "  (cnn1): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (relu1): ReLU()\n",
      "  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (cnn2): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (relu2): ReLU()\n",
      "  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=89888, out_features=512, bias=True)\n",
      "  (relu5): ReLU()\n",
      "  (fc2): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (output): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = LeNet()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 16, 220, 220]           1,216\n",
      "              ReLU-2         [-1, 16, 220, 220]               0\n",
      "         MaxPool2d-3         [-1, 16, 110, 110]               0\n",
      "            Conv2d-4         [-1, 32, 106, 106]          12,832\n",
      "              ReLU-5         [-1, 32, 106, 106]               0\n",
      "         MaxPool2d-6           [-1, 32, 53, 53]               0\n",
      "            Linear-7                  [-1, 512]      46,023,168\n",
      "            Linear-8                    [-1, 2]           1,026\n",
      "           Softmax-9                    [-1, 2]               0\n",
      "================================================================\n",
      "Total params: 46,038,242\n",
      "Trainable params: 46,038,242\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 19.47\n",
      "Params size (MB): 175.62\n",
      "Estimated Total Size (MB): 195.67\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "model = model.cuda()\n",
    "summary(model, input_size=(3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 46,038,242 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"The model has {count_parameters(model):,} trainable parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def train_model(model, dataloader_dict, criterion, optimizer, num_epoch):\n",
    "    since = time.time()\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    for epoch in range(num_epoch):\n",
    "        print(\"Epoch {}/{}\".format(epoch+1, num_epoch))\n",
    "        print('-'*20)\n",
    "        \n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "                \n",
    "            epoch_loss = 0.0\n",
    "            epoch_corrects = 0\n",
    "            \n",
    "            for inputs, labels in tqdm(dataloader_dict[phase]):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                    epoch_loss += loss.item() * inputs.size(0)\n",
    "                    epoch_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "                epoch_loss = epoch_loss / len(dataloader_dict[phase].dataset)\n",
    "                epoch_acc = epoch_corrects.double() / len(dataloader_dict[phase].dataset)\n",
    "                \n",
    "                print('{} Loss: {:,4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "                \n",
    "                if phase == 'val' and epoch_acc > best_acc:\n",
    "                    best_acc = epoch_acc\n",
    "                    best_model_wts = model.state_dict()\n",
    "                    \n",
    "    time_elapsed = time.time() - since\n",
    "    print(\"Training complete in {:.0f}m {:.0f}s\".format(\n",
    "        time_elapsed // 60, time_elapsed % 60\n",
    "    ))\n",
    "    print(\"Best val acc: {:4f}\".format(best_acc))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sangil\\AppData\\Local\\Temp\\ipykernel_16904\\346366194.py:19: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for inputs, labels in tqdm(dataloader_dict[phase]):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be45947e86bb4b81b3788822cdf89092",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[87], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m num_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m train_model(model, dataloader_dict, criterion, optimizer, num_epoch)\n",
      "Cell \u001b[1;32mIn[86], line 21\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, dataloader_dict, criterion, optimizer, num_epoch)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m tqdm(dataloader_dict[phase]):\n\u001b[0;32m     20\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 21\u001b[0m     labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     22\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(phase \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "num_epoch = 10\n",
    "\n",
    "model = train_model(model, dataloader_dict, criterion, optimizer, num_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sangil",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
